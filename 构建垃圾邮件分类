#1.1：了解数据集
#是2列没有命名数据集
#第一列有两个值："ham",表示信息不是垃圾信息，以及"spam",表示信息是垃圾信息
#第二列是被分类的信息的文本内容
#说明：
#使用read_table方法可以讲数据集导入pandas 数据帧。因为这是一个用制表符分隔的数据集，因此我们将使用"\t"作为"sep"参数的值，
#表示这种分隔格式
#此外，通过为read_table()的"names"参数制定列表["label","sms_message"],重命名列
#用新的列明输出数据帧的前五个值

import pandas as pd
#Dataset from -http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection
df=pd.read_table("smsspamcollection/SMSSpamCollection",
                 sep="\t",
                 header=None,
                 names=["labels","sms_message"])
#Output printing out first 5 columns
df.head()

#1.2数据预处理
#了解了数据结构之后，现将标签转换为二元变量，0表示"ham"，1表示"spam",方便计算
#原因在于scikit-learn处理的输入方式只有数字，如果标签值保留为字符串，scikit-learn会自己转换为未知的浮点值。
#说明
#使用映射方法将”标签“列中的值转换为数字值，{"ham":0,"spam":1}这样将"ham"值映射为0，将"spam"值映射为1
#此外，为了知道正在处理的数据集有多大，使用”shape“输出行数和列数

df["label"]=df.label.map({"ham":0,"spam":1})
print(df.shape)
df.head

#2.1Bag of words
#数据集中有大量文本数据（5572行数据）。大多数机器学习算法都要求传入的输入是数字数据，而电子邮件、信息通常都是文本。
#Bag of Words(BOW)这个概念，它是用来表示要处理的问题具有”大量单词“或很多文本数据。BOW的基本概念是拿出一段文本，
#计算该文本中单词的出现频率。注意：BOW平等地对待每个单词，单词出现的顺序并不重要
#将文档集合转换成矩阵，每个文档是一行，每个单词（令牌）是一列，对应的（行，列）值是每个单词或令牌在此文档中出现的频率
#要处理这一步，将使用sklearns count vectorizer方法，该方法的作用如下所示：
#他会令牌化字符串（将字符串划分为单个单词）并未每个令牌设定一个整型ID
#他会计算每个令牌的出现次数
#请注意
#1.countvectorizer方法会自动将所有令牌化单词转换为小写形式，避免区分”HE“和"he"等单词。为此，他会使用参数lowercase,该参数默认值为True
#2.他还会忽略所有标点符号，避免区分后面有标点的单词（例如”hello!“）和前后没有标点的同一单词（例如"hello"）。为此，他会使用参数token_pattern,
    该参数使用默认正则表达式选择具有2个或多个字母数字字符的令牌。
#3.要注意的第三个参数是stop_words.停用此是指某个语言中最常用的字词，包括”am“,"an","and","the"等。通过将参数值设为english,
   countvectorizer将自动忽略（输入文本中）出现在scikti-learn中的内置英语停用此列表中的所有单词。这非常有用，因为当尝试查找表明是
   垃圾内容的某些单词时，停用词会使结论出现偏差。
   #2.2从头实现Bag of Words
   #第一步，将所有字符串转换成小写形式
documents=["Hello,how are you!",
            "Win money,win from home.",
            "Call me now.",
            "Hello,Call hello you tomorrow?" ]
from sklearn.feature_extraction.text import CountVectorizer
vectorizer= CountVectorizer()
X=vectorizer.fit_transform(documents)
print(vectorizer.get_feature_names())
print(X.toarray())


#说明
#将文档集合中的所有字符串转换为小写形式，将他们保存到叫做"lower_case_documents"的列表中，你可以使用lower()方法在python中将字符串
#转化为小写形式
documents=["Hello,how are you!",
           "Win money,win from home.",
           "Call me now.",
           "Hello,Call hello you tomorrow?"]
lower_case_documents=[]
for i in documents：
    lower_case_documents.append(i.lower())
print(lower_case_documents)

#删除所有标点符号
#删除文档集合中的字符串中的所有标点符号，将他们保存在叫做"dans_punctuation_documents"列表中
sans_punctuation_documents=[]
import string
for i in lower_case_documents:
    sans_punctuation_documents.append(i.translate(str.maketrans("","",string.punctuation)))
print(sans_punctuation_documents)


#令牌化
#令牌化文档集合中的句子是指使用分隔符将句子拆分为单个单词。分隔符指定了我们将使用哪个字符来表示单词的开始和结束位置
#例如，我们可以使用一个空格作为我们的文档集合的单词分隔符
#说明
#使用split()方法令牌化"sans_punctuation_documents"中存储的字符串，并将最终文档集合存储在叫做"preprocessed_documents"的列表中
preprocessed_documents=[]
for i in sans_punctuation_documents:
    preprocessed_documents.append(i.split(" "))
print(preprocessed_documents)

#计算频率
#已经获得所需格式的文档集合，现在可以数出每个单词在文档集合的每个文档中出现的次数了，为此，我们将使用python collections库中的Counter方法
# Counter 会数出列表中每项的出现次数，并返回一个字典，键是被数的项目，相应的值是该项目在列表中的计数。
#说明
#使用counter()方法和作为输入的preprocessed_documents创建一个字典，键是每个文档中的每个单词，相应的值是该单词的出现的频率。
#将每个counter字典当作项目另存到一个叫做frequency_list的列表中。
frequency_list=[]
import pprint
from collections import Counter
for i in preprocessed_documents:
   frequency_count=Counter(i)
   frequency_list.append(frequency_count)
pprint.pprint(frequency_list)

#2.3在scikit-learn中实现Bag of Words
#已经从头实现了BOW概念，并使用scikit-learn以简洁的方式实现这一流程，我们将使用在上一步用到的相同文档集合
documents=["Hello,how are you!",
           "Win money,win from home",
           "Call me now",
           ”Hello,Call hello you tomorrow?“]
#说明：导入sklearn.feature_extraction.test.Countcetorizer方法创建一个实例，命名为"count_vector"
from sklearn.feature_extraction.text import CountVectorizer
count_vector=CountVectorizer（）

#使用CountVectorizer预处理数据
#在2.2步，从头实现了可以首先清理数据的CountVectorizer（）方法。清理过程包括将所有数据转换为小写形式，并删除所有标点符号。
#CountVectorizer（）具有某些可以帮助我们完成这些步骤的参数，这些参数包括：
#lowercase=True ,它会将所有文本都转换为小写形式
#token_pattern， 参数具有默认正则表达式值 ，它会忽略所有标点符号并将他们当作分隔符，并将长度大于等于2的字母字符串当作单个令牌或单词。
#stop_words参数如果设为english，将从文档集合中删除与scikit-learn中定义的英语停用此列表匹配的素有单词，考虑到数据集规模不大，
#   并且处理的是信息，并不是电子邮件这样庞大文本来源，因此我们将不设置此参数值
#可以通过如下所示输出 count__vector对象，查看该对象的所有参数值：
print(count__vector)

#说明：使用fit()将你的文档数据集与 CountVectorizer对象进行拟合，并使用get_feature_names()方法获得被归类为特征的单词列表
count_vector.fit(documents)
count_vector.get_feature_names()
#get_feature_names()方法会返回此数据集的特征名称，即组成"documents"词汇表的单词集合

#说明
#创建一个矩阵，行是4个文档中每个文档的行，列是每个单词，对应的值（行，列）是该单词（在列中）在特定文档（在行中）中出现的频率。
#为此，你可以使用transform()方法并传入文档数据集作为参数。transform()方法会返回一个Numpy整数矩阵，你可以使用toarray()将基转换为
   数组，称之为”doc_array“
doc_array=count_vector.transform(documents).toarray()
doc_array
#现在，对于单词在文档中的出现频率，我们已经获得了整洁的文档表达形式。为了方便理解，下一步将此数组转换为数据帧，并相应地列命名。

#数模那个：将我们获得并加载到”doc_array“中的数组转换为数据帧，并将列名设为单词名称（你之前使用get_feature_names（）计算了名称）
#将该数帧命名为"frequency_matrix"
frequency_matrix=pd.DataFrame(doc_array,columns=count_vector.get_feature_names())
frequency_matrix
#直接使用该方法的一个潜在问题是如果我们的文本数据集非常庞大（假设有一大批新闻文章或电子邮件数据），犹豫语言本身的问题，肯定有某些值比
#   其他值更常见。例如 "is","the","an"等单词，代词，语法结构等会使矩阵出现偏斜并影响到分析结果。
#有几种方式可以减轻这种情况。一种方式是使用stop_words参数并将其值设为english。这样会自动忽略scikit-learn中内置英语停用词列表中出现的单词。所有



