#1.1：了解数据集
#是2列没有命名数据集
#第一列有两个值："ham",表示信息不是垃圾信息，以及"spam",表示信息是垃圾信息
#第二列是被分类的信息的文本内容
#说明：
#使用read_table方法可以讲数据集导入pandas 数据帧。因为这是一个用制表符分隔的数据集，因此我们将使用"\t"作为"sep"参数的值，
#表示这种分隔格式
#此外，通过为read_table()的"names"参数制定列表["label","sms_message"],重命名列
#用新的列明输出数据帧的前五个值

import pandas as pd
#Dataset from -http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection
df=pd.read_table("smsspamcollection/SMSSpamCollection",
                 sep="\t",
                 header=None,
                 names=["labels","sms_message"])
#Output printing out first 5 columns
df.head()

#1.2数据预处理
#了解了数据结构之后，现将标签转换为二元变量，0表示"ham"，1表示"spam",方便计算
#原因在于scikit-learn处理的输入方式只有数字，如果标签值保留为字符串，scikit-learn会自己转换为未知的浮点值。
#说明
#使用映射方法将”标签“列中的值转换为数字值，{"ham":0,"spam":1}这样将"ham"值映射为0，将"spam"值映射为1
#此外，为了知道正在处理的数据集有多大，使用”shape“输出行数和列数

df["label"]=df.label.map({"ham":0,"spam":1})
print(df.shape)
df.head

#2.1Bag of words
#数据集中有大量文本数据（5572行数据）。大多数机器学习算法都要求传入的输入是数字数据，而电子邮件、信息通常都是文本。
#Bag of Words(BOW)这个概念，它是用来表示要处理的问题具有”大量单词“或很多文本数据。BOW的基本概念是拿出一段文本，
#计算该文本中单词的出现频率。注意：BOW平等地对待每个单词，单词出现的顺序并不重要
#将文档集合转换成矩阵，每个文档是一行，每个单词（令牌）是一列，对应的（行，列）值是每个单词或令牌在此文档中出现的频率
#要处理这一步，将使用sklearns count vectorizer方法，该方法的作用如下所示：
#他会令牌化字符串（将字符串划分为单个单词）并未每个令牌设定一个整型ID
#他会计算每个令牌的出现次数
#请注意
#1.countvectorizer方法会自动将所有令牌化单词转换为小写形式，避免区分”HE“和"he"等单词。为此，他会使用参数lowercase,该参数默认值为True
#2.他还会忽略所有标点符号，避免区分后面有标点的单词（例如”hello!“）和前后没有标点的同一单词（例如"hello"）。为此，他会使用参数token_pattern,
    该参数使用默认正则表达式选择具有2个或多个字母数字字符的令牌。
#3.要注意的第三个参数是stop_words.停用此是指某个语言中最常用的字词，包括”am“,"an","and","the"等。通过将参数值设为english,
   countvectorizer将自动忽略（输入文本中）出现在scikti-learn中的内置英语停用此列表中的所有单词。这非常有用，因为当尝试查找表明是
   垃圾内容的某些单词时，停用词会使结论出现偏差。
   #2.2从头实现Bag of Words
   #第一步，将所有字符串转换成小写形式
documents=["Hello,how are you!",
            "Win money,win from home.",
            "Call me now.",
            "Hello,Call hello you tomorrow?" ]
from sklearn.feature_extraction.text import CountVectorizer
vectorizer= CountVectorizer()
X=vectorizer.fit_transform(documents)
print(vectorizer.get_feature_names())
print(X.toarray())


#说明
#将文档集合中的所有字符串转换为小写形式，将他们保存到叫做"lower_case_documents"的列表中，你可以使用lower()方法在python中将字符串
#转化为小写形式
documents=["Hello,how are you!",
           "Win money,win from home.",
           "Call me now.",
           "Hello,Call hello you tomorrow?"]
lower_case_documents=[]
for i in documents：
    lower_case_documents.append(i.lower())
print(lower_case_documents)

#删除所有标点符号
#删除文档集合中的字符串中的所有标点符号，将他们保存在叫做"dans_punctuation_documents"列表中
sans_punctuation_documents=[]
import string
for i in lower_case_documents:
    sans_punctuation_documents.append(i.translate(str.maketrans("","",string.punctuation)))
print(sans_punctuation_documents)


#令牌化
#令牌化文档集合中的句子是指使用分隔符将句子拆分为单个单词。分隔符指定了我们将使用哪个字符来表示单词的开始和结束位置
#例如，我们可以使用一个空格作为我们的文档集合的单词分隔符
#说明
#使用split()方法令牌化"sans_punctuation_documents"中存储的字符串，并将最终文档集合存储在叫做"preprocessed_documents"的列表中
preprocessed_documents=[]
for i in sans_punctuation_documents:
    preprocessed_documents.append(i.split(" "))
print(preprocessed_documents)

#计算频率
#已经获得所需格式的文档集合，现在可以数出每个单词在文档集合的每个文档中出现的次数了，为此，我们将使用python collections库中的Counter方法
# Counter 会数出列表中每项的出现次数，并返回一个字典，键是被数的项目，相应的值是该项目在列表中的计数。
#说明
#使用counter()方法和作为输入的preprocessed_documents创建一个字典，键是每个文档中的每个单词，相应的值是该单词的出现的频率。
#将每个counter字典当作项目另存到一个叫做frequency_list的列表中。
frequency_list=[]
import pprint
from collections import Counter
for i in preprocessed_documents:
   frequency_count=Counter(i)
   frequency_list.append(frequency_count)
pprint.pprint(frequency_list)

#2.3在scikit-learn中实现Bag of Words
#已经从头实现了BOW概念，并使用scikit-learn以简洁的方式实现这一流程，我们将使用在上一步用到的相同文档集合
documents=["Hello,how are you!",
           "Win money,win from home",
           "Call me now",
           ”Hello,Call hello you tomorrow?“]
#说明：导入sklearn.feature_extraction.test.Countcetorizer方法创建一个实例，命名为"count_vector"
from sklearn.feature_extraction.text import CountVectorizer
count_vector=CountVectorizer（）

#使用CountVectorizer预处理数据
#在2.2步，从头实现了可以首先清理数据的CountVectorizer（）方法。清理过程包括将所有数据转换为小写形式，并删除所有标点符号。
#CountVectorizer（）具有某些可以帮助我们完成这些步骤的参数，这些参数包括：
#lowercase=True ,它会将所有文本都转换为小写形式
#token_pattern， 参数具有默认正则表达式值 ，它会忽略所有标点符号并将他们当作分隔符，并将长度大于等于2的字母字符串当作单个令牌或单词。
#stop_words参数如果设为english，将从文档集合中删除与scikit-learn中定义的英语停用此列表匹配的素有单词，考虑到数据集规模不大，
#   并且处理的是信息，并不是电子邮件这样庞大文本来源，因此我们将不设置此参数值
#可以通过如下所示输出 count__vector对象，查看该对象的所有参数值：
print(count__vector)

#说明：使用fit()将你的文档数据集与 CountVectorizer对象进行拟合，并使用get_feature_names()方法获得被归类为特征的单词列表
count_vector.fit(documents)
count_vector.get_feature_names()
#get_feature_names()方法会返回此数据集的特征名称，即组成"documents"词汇表的单词集合

#说明
#创建一个矩阵，行是4个文档中每个文档的行，列是每个单词，对应的值（行，列）是该单词（在列中）在特定文档（在行中）中出现的频率。
#为此，你可以使用transform()方法并传入文档数据集作为参数。transform()方法会返回一个Numpy整数矩阵，你可以使用toarray()将基转换为
   数组，称之为”doc_array“
doc_array=count_vector.transform(documents).toarray()
doc_array
#现在，对于单词在文档中的出现频率，我们已经获得了整洁的文档表达形式。为了方便理解，下一步将此数组转换为数据帧，并相应地列命名。

#数模那个：将我们获得并加载到”doc_array“中的数组转换为数据帧，并将列名设为单词名称（你之前使用get_feature_names（）计算了名称）
#将该数帧命名为"frequency_matrix"
frequency_matrix=pd.DataFrame(doc_array,columns=count_vector.get_feature_names())
frequency_matrix
#直接使用该方法的一个潜在问题是如果我们的文本数据集非常庞大（假设有一大批新闻文章或电子邮件数据），犹豫语言本身的问题，肯定有某些值比
#   其他值更常见。例如 "is","the","an"等单词，代词，语法结构等会使矩阵出现偏斜并影响到分析结果。
#有几种方式可以减轻这种情况。一种方式是使用stop_words参数并将其值设为english。这样会自动忽略scikit-learn中内置英语停用词列表中出现的单词。所有


#3.1训练集和测试集
#知道如何处理Bag of Words问题，现在回到数据集并继续分析工作。第一步是将数据集拆分为训练集和测试集，以便稍后测试模型。
#说明
#通过在sklearn中使用train_test_split方法，将数据集拆分为训练集和测试集。使用以下变量拆分数据：
# X_train是 "sms_message"列的训练数据
# y_train是 "label"列的训练数据
# X_test 是 "sms_message"列的测试数据
# y_test 是 "label" 列的测试数据，输出每个训练数据和测试数据的行数。
#NOTE:sklearn.cross_validation will be deprecated soon to sklearn.model_selection
#split into training and testing sets
#USE from sklearn.model_selection import train_test_split to avoid seeing deprecation warning.
from sklearn.cross_validation import train_test_split
X_train,X_test,y_train,y_test=train_test_split(df["sms_message"],
                                               df["label"],
                                               random_state=1)
print("Number of rows in the total set:{}".format(df.shape[0]))
print("Number of rows in the training set:{}".format(X_train.shape[0]))
print("Number of rows in the test set:{}".format(X_test.shape[0]))



#3.2对数据集应用Bag of Words 流程
#现在已经拆分了数据，下个目标是按照第二部：Bag of words 中的步骤操作，并将数据转换为期望的矩阵格式。为此，我们将像之前一样使用CountVectorizer()
#首先，需要对CountVectorizer()拟合训练数据（X_train）并返回矩阵
#其次，需要转换测试数据（X_test）以返回矩阵
#注意：
#X_train是数据集中"sms_message"列的训练数据，将使用此数据训练模型
#X_test是”sms_message“列的测试数据，我们将使用该数据（转换为矩阵后）进行预测。然后在后面的步骤中将这些预测与y_test进行比较

#Instantiate the CountVectorizer method
count_vector=COuntVectorizer()

#Fit the training data and then return the matrix
training_data=count_vector.fit_transform(X_train)

#Transform testing data and return the matrix.NOte we are not fitting the testing data into the CountVectorizer()
testing_data=count_vector.transform(X_test)

#4.1从头实现贝叶斯定理
#数据集已经是所需的格式，现在可以进行任务的下一步，即研究用来做出预测并将信息分类为垃圾信息或非来及信息的算法。
#贝叶斯定理与相关事件有关的其他的概率计算该时间发生的概率。它由先验概率（已经知道的概率或提供给我们的概率）和后验概率（希望用先验部分计算的概率）组成

#加黑色要根据某人接受糖尿病检测后获得阳性结果计算此人有糖尿病的概率。在医学领域，此类概率非常重要。
#我们假设：
#P(D)是某人环游糖尿病的概率，值为0.01，换句话说，普通人群中有1%的人患有糖尿病。
#P(Pos)：是获得阳性测试结果的概率
#P（Neg）:是获得阴性测试结果的概率
#P(Pos|D):是本身有糖尿病并且获得阳性测试结果的概率，值为0.9，换句话说，该测试在90%的情况下是正确的。亦成为敏感性或真正例率
#P(Neg|~D)：是本身没有糖尿病并且获得阴性测试结果的概率，值为0.9，因此在90%的情况下是正确的，亦成为特异值或真负例率
#则beiyes公式为    P(A|B)=P(B|A)P(A)/P(B)
#P(A):A 独立发生的先验概率。在我们的示例中为P(D)，该值已经提供给我们了。
#P(B)：B独立发生的先验概率。在我们的示例中为P(Pos)
#P(A|B)：在给定B的情况下A发生的后验概率，在我们的示例中P(D|Pos)，即某人的测试结果为阳性时患有糖尿病的概率。这是我们需要计算的值。
#P(B|A)：在给定A的情况下B可能发生的概率。在我们的示例中为P(Pos|D)，该值已经提供给我们了。
#将这些值带入beiyes定理公式中：
P(D|Pos)=P(D)*P(Pos|D)/P(Pos)
#获得阳性测试结果P(Pos)的概率可以使用敏感性sensitiity和特异性specificity来计算，如下所示：
P(Pos)=[P(D)*Sensitivity]+[P(~D)*(1-Specificity)]

#Instructions:
#Calculate probability of getting a positive test tesult,P(Pos)
#P(D)
p_diabetes=0.01
#p(~D)
p_no_diabetes=0.99
#Sensitivity or P(Pos|D)
p_pos_diabetes=0.9
#specificity or P(Neg|~D)
p_neg_no_diabetes=0.9
#P(Pos)
p_pos=(p_diabetes*p_pos_diabetes)+(p_no_diabetes*(1-p_neg_no_diabetes))
print("The probability of getting a positive test result P(Pos) is :{}".format(p_pos))

#我们可以利用所有这些信息计算后验概率，如下所示：某人测试结果为阳性时环游糖尿病的概率为：
#P(D|Pos)=(P(D)*Sensitivity)/P(Pos)
#某人测试结果为阳性时没有糖尿病的概率为：
#P(~D|Pos)=(P(~D)*(1-specificity))/P(Pos)
#后验概率和将始终为1
#Instructions:
#Compute the probability of an individual having diabetes,given that,that individual got a positive test result.

#P(D|Pos)
p_diabetes_pos=(p_diabetes*p_pos_diabetes)/p_pos
print("Probability of an individual having diabetes,given that individual got a positive test result is:".format(p_diabetes_pos))

#Instructions:
#Compute the probability of an individual not having diabetes,given that,that individual got a positive test result
#In other words,compute P(~D|Pos)
#The formula is: P(~D|Pos)=P(~D)*P(Pos|~D)/P(Pos)
#Note that P(Pos|~D) canbe computed as 1-P(Neg|~D)
#Therefore: P(Pos|~D)=p_pos_no_diabetes=1-0.9=0.1
#P(Pos|~D)
p_pos_no_diabetes=0.1
#P(~D|Pos)
p_no_diabetes_pos=(p_no_diabetes*p_pos_no_diabetes)/p_pos
print("Probability of an individual not having diabetes,given that individual got a positive test result is:".formate(p_no_daibetes_pos))

#朴素beiyes中的朴素一词实际上是指，算法在进行预测时使用的特征相互之间是独立的，但实际上并非始终这样。在我们的糖尿病示例中，我们只考虑了一个特征，即测试结果。
#假设我们添加了另一个特征”锻炼“。假设此特征具有二元值0和1，0表示某人一周的锻炼时间不超过2天，1表示某人一周的锻炼时间超过2天。如果我们要
#同时使用这两个特征（即测试结果和”锻炼“特征的值）计算最终概率，beiyes定理将不可行，朴素beiyes是定理的一种延伸，假设所有特征相互是独立的

#4.2从头实现朴素beiyes
#Jill Stein 提到”自由“的概率：0.1------>P(F|J)
#Jill Stein 提到”移民“的概率：0.1------>P(I|J)
#Jill Stein 提到”环境“的概率：0.8------>P(E|J)

#Gary Johnson 提到”自由“的概率：0.7---->P(E|G)
#Gary Johnson 提到”移民“的概率：0.2---->P(I|G)
#Gary Johnson 提到”环境“的概率：0.1---->P(E|G)

#假设Jill Stein 发表演讲的概率P(J)是0.5，Gary Johnson也是 P(G)=0.5
#此时，要计算Jill Stein提到 ”自由“和”移民“的概率：
      P(y|x1,...,xn)=P(y)P(x1,...,xn|y)/P(x1,...,xn)
#该公式中，y是分类变量，即候选人的姓名，x1到xn是特征向量，即单个单词，该定理假设每个特征向量或单词（xi）相互之间是独立的
#为了详细讲解该公式，需要计算以下后验概率
#P(J|F,I):Jill Stein 提到”自由“和”移民“的概率
#根据上述公式，可以进行以下计算：P(G|F,I)=(P(G)*P(F|G)*P(I|G))/P(F,I)

#P(J)
p_j=0.5

#P(F|J)
p_j_f=0.1

#P(I|J)
p_j_i=0.1

#p_j_text=p_j*p_j_f*p_j_i
print(p_j_text)

#P(G)
p_g=0.5

#P(F|G)
p_g_f=0.7

#P(I|G)
p_g_i=0.2

p_g_text=p_g*p_g_f*p_g_i
print(p_g_text)

p_f_i=p_j_text + p_g_text
print("Probability of words freedom and immigration being said are:".format(p_f_i))

p_j_fi=p_j_text/p_f_i
print("The probability of Jill Stein saying the words Freedom and Immigration:".format(p_j_fi))

p_g_fi=p_g_text/p_f_i
print("The probability of Gary Johnson saying the words Freedom and Immigration:".format(p_g_fi))

5.使用scikit-learn实现朴素beiyes
from sklearn.naive_bayes import MultinomialNB
naive_bayes=NultinomialNB()
naive_bayes.fit(training_data,y_train)
predictions=naive_bayes.predict(testing_data)

6评估模型
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print('Accuracy score: ', format(accuracy_score(y_test,predictions)))
print('Precision score: ', format(precision_score(y_test,predictions)))
print('Recall score: ', format(recall_score(y_test,predictions)))
print('F1 score: ', format(f1_score(y_test,predictions)))















































































